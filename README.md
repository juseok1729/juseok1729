### Juseok, Oh [*@ojkk371*](https://blog.naver.com/ojkk371)
<p align="left"> <img src="https://komarev.com/ghpvc/?username=ojkk371&color=brightgreen" alt="ojkk371" /> </p>

### âš¡ Major : Electric Engineering

#
### ğŸ”­ Career
- Joined the CRESPRIT. ,Bundang-gu, Korea, as a Deep Learning Research Engineer, in 2020.

### ğŸ¤” Study
#### 1. ResNet
  - **BasicBlock**
    - ResNet18 : [2, 2, 2, 2]
    - ResNet34 : [3, 4, 6, 3]
  - **Bottleneck**
    - ResNet50 : [3, 4, 6, 3]
    - ResNet101 : [3, 4, 23, 3]
    - ResNet152 : [3, 8, 36, 3]
  
#### 2. ResNeXt
  - **Bottleneck**
    - ResNeXt-50 32x4d : [3, 4, 2, 3]
      - group : 32
      - width_per_group : 4
    - ResNeXt-101 32x8d : [3, 4, 23, 3]
      - group : 32
      - width_per_group : 8
      
      
#### 3. Optimizer  
  - **ì˜µí‹°ë§ˆì´ì € ë°œì „ê³¼ì •**  
![](https://github.com/ojkk371/ojkk371/blob/master/optim.PNG?raw=true)  
- **Gradient**ë¥¼ ìˆ˜ì •í•œ **Momentum, NAG**  
  GD â†’ SGD â†’ ***Momentum***  
  â†’ Adam â†’ Nadam  
  â†’ ***NAG*** â†’ Nadam  
- **Learning rate**ë¥¼ ìˆ˜ì •í•œ **Adagrad, RMSProp, AdaDelta**   
  GD â†’ SGD â†’ ***Adagrad***  
  â†’ ***RMSProp*** â†’ Adam  
  â†’ ***AdaDelta***  

 

ë‘ ì¢…ë¥˜ì˜ *ì¥ì ì„ í•©í•œ* **Adam, Nadam**
  
#
### ğŸŒ± Interest
- **Crawling**
    - [High Resolution Image Crawling](https://github.com/ojkk371/Image-crawler)    
- **Image Recognition using Deep Learning**    
    - [Object Detection](https://github.com/ojkk371/Object-Detection)
    - ResNet
    - Re-identification
    - Pose-estimation
    - Tracking & Counting
- **Vision**
    - [OpenCV (i/o)](https://crmn.tistory.com/49?category=785177)
    - [OpenCV (pre-processing)](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_table_of_contents_imgproc/py_table_of_contents_imgproc.html)
    - [OpenCV (Camera calibration)](https://darkpgmr.tistory.com/32)
    - [Image augmentation (imgaug)](https://github.com/aleju/imgaug)
- **Financial Engineering**
    - [Stock Data Crawler](https://github.com/ojkk371/Stock-datareader)
    - Time series data
    - BlockChain
    - Futures
    - Stock
- **Visualize**
    - [Word Cloud](https://tariat.tistory.com/854)
- **Paper-review**
    - [Faster_R-CNN](https://github.com/ojkk371/Paper-review/blob/master/Faster_R-CNN/README.md)  
       - [[arxiv] Towards Real-Time Object Detection with Region Proposal Networks](https://arxiv.org/pdf/1506.01497.pdf)
    - [YOLOv1](https://github.com/ojkk371/Paper-review/blob/master/YOLOv1/README.md)  
       - [[arxiv] Unified, Real-Time Object Detection](https://arxiv.org/pdf/1506.02640.pdf)
    - [Focal Loss](https://github.com/ojkk371/Paper-review/blob/master/Focal_Loss/README.md)  
       - [[arxiv] Focal Loss for Dense Object Detection](https://arxiv.org/pdf/1708.02002.pdf)
    - [EfficientNet and EfficientDet](https://github.com/ojkk371/Paper-review/blob/master/EfficientDet/README.md)  
       - [[arxiv] Scalable and Efficient Object Detection](https://arxiv.org/pdf/1911.09070.pdf)
    - [EfficientNet](https://github.com/ojkk371/Paper-review/blob/master/EfficientNet/README.md)  
       - [[arxiv] Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/pdf/1905.11946.pdf%E2%80%8Barxiv.org)
    
#
### ğŸ‘¯ Hobby
- **Research** ([Deep learning for ENSO forecasts](https://github.com/ojkk371/Deep-learning-for-ENSO-forecasts), [Paper](https://github.com/ojkk371/Paper-review))
- **Bicycle** (Travel)
- **Art** (Sketch, Sculpture)
- **CAD 2D/3D** (Interior, Architectural, Product design)
- **Biology** (Fresh water, Sea, Ecology)
#
### ğŸ“« How to reach me
- Instagram : https://www.instagram.com/juseok_93/
- Email : juseok.oh@cresprit.com


<!--
**ojkk371/ojkk371** is a âœ¨ _special_ âœ¨ repository because its `README.md` (this file) appears on your GitHub profile.

Here are some ideas to get you started:

- ğŸ”­ Iâ€™m currently working on ...
- ğŸŒ± Iâ€™m currently learning ...
- ğŸ‘¯ Iâ€™m looking to collaborate on ...
- ğŸ¤” Iâ€™m looking for help with ...
- ğŸ’¬ Ask me about ...
- ğŸ“« How to reach me: ...
- ğŸ˜„ Pronouns: ...
- âš¡ Fun fact: ...
-->
